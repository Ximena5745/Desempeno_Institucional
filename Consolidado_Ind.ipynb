{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYN0R5NmAnh95lROUd02Vg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ximena5745/Desempeno_Institucional/blob/main/Consolidado_Ind.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6dPMghJ8Ign1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import ast\n",
        "import pandas as pd\n",
        "from html import unescape\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Ximena5745/Desempeno_Institucional.git"
      ],
      "metadata": {
        "id": "yLXHriEzH2z8",
        "outputId": "456987bd-8903-408e-8944-c10c0afa9450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Desempeno_Institucional' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/Desempeno_Institucional/Data\")"
      ],
      "metadata": {
        "id": "4-rTCGb2Kilp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Cargar todos los Excel del directorio, excluyendo cualquier archivo de consolidado previo\n",
        "archivos_excel = [f for f in (glob.glob(\"*.xlsx\") + glob.glob(\"*.xls\")) if \"consolidado\" not in f.lower()]\n"
      ],
      "metadata": {
        "id": "CCmPCuhrLCjN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = []\n",
        "for archivo in archivos_excel:\n",
        "    try:\n",
        "        df_tmp = pd.read_excel(archivo)\n",
        "        df_tmp[\"Archivo\"] = archivo\n",
        "        dfs.append(df_tmp)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ No se pudo leer {archivo}: {e}\")\n",
        "\n",
        "if not dfs:\n",
        "    raise RuntimeError(\"No se encontraron archivos Excel para consolidar.\")\n",
        "\n",
        "# 4) Unir y estandarizar columnas\n",
        "df_todos = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
        "df_todos.columns = df_todos.columns.str.strip().str.lower()\n",
        "\n",
        "# 5) Limpieza pedida\n",
        "cols_a_eliminar = [\"dependencia\", \"incidencia\", \"objectivos_relacionados\"]\n",
        "df_todos = df_todos.drop(columns=[c for c in cols_a_eliminar if c in df_todos.columns], errors=\"ignore\")\n",
        "\n",
        "if \"fecha\" in df_todos.columns:\n",
        "    df_todos = df_todos[df_todos[\"fecha\"].notna() & (df_todos[\"fecha\"].astype(str).str.strip() != \"\")]\n",
        "\n",
        "if \"clasificacion\" in df_todos.columns:\n",
        "    df_todos[\"clasificacion\"] = df_todos[\"clasificacion\"].astype(str).map(lambda s: unescape(s))\n",
        "    df_todos[\"clasificacion\"] = df_todos[\"clasificacion\"].replace({\"Estrat&eacute;gico\": \"Estratégico\"})\n"
      ],
      "metadata": {
        "id": "Qq9EHtX77GUG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Funciones auxiliares\n",
        "# -------------------------\n",
        "def parse_variables_cell(x):\n",
        "    \"\"\"Convierte 'variables' de Serie Única a lista de dicts.\"\"\"\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return []\n",
        "    if isinstance(x, list):\n",
        "        return [ {\"valor\": i.get(\"valor\"), \"nombre\": i.get(\"nombre\"), \"simbolo\": i.get(\"simbolo\")} for i in x if isinstance(i, dict) ]\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in {\"nan\", \"none\", \"null\", \"[]\"}:\n",
        "        return []\n",
        "    try:\n",
        "        pyobj = ast.literal_eval(s.replace(\"null\", \"None\").replace(\"true\", \"True\").replace(\"false\", \"False\"))\n",
        "        if isinstance(pyobj, list):\n",
        "            return [ {\"valor\": i.get(\"valor\"), \"nombre\": i.get(\"nombre\"), \"simbolo\": i.get(\"simbolo\")} for i in pyobj if isinstance(i, dict) ]\n",
        "    except Exception:\n",
        "        pass\n",
        "    bloques = re.findall(r\"\\{[^{}]*\\}\", s)\n",
        "    out = []\n",
        "    for b in bloques:\n",
        "        m_val = re.search(r\"[\\'\\\"]?valor[\\'\\\"]?\\s*:\\s*([\\-+]?\\d+(?:[.,]\\d+)?)\", b, flags=re.IGNORECASE)\n",
        "        m_nom = re.search(r\"[\\'\\\"]?nombre[\\'\\\"]?\\s*:\\s*[\\'\\\"](.*?)[\\'\\\"]\", b, flags=re.IGNORECASE)\n",
        "        m_sim = re.search(r\"[\\'\\\"]?simbolo[\\'\\\"]?\\s*:\\s*[\\'\\\"](.*?)[\\'\\\"]\", b, flags=re.IGNORECASE)\n",
        "        valor = None\n",
        "        if m_val:\n",
        "            v = m_val.group(1).replace(\",\", \".\")\n",
        "            try:\n",
        "                valor = float(v)\n",
        "            except Exception:\n",
        "                valor = v\n",
        "        nombre = m_nom.group(1) if m_nom else None\n",
        "        simbolo = m_sim.group(1) if m_sim else None\n",
        "        if any([m_val, m_nom, m_sim]):\n",
        "            out.append({\"valor\": valor, \"nombre\": nombre, \"simbolo\": simbolo})\n",
        "    return out\n",
        "\n",
        "def parse_multiserie_cell(x):\n",
        "    \"\"\"Convierte celda de 'series' a lista de dicts con meta, nombre, resultado y variables.\"\"\"\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return []\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    try:\n",
        "        return ast.literal_eval(str(x))\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# -------------------------\n",
        "# 6) Serie Única\n",
        "# -------------------------\n",
        "if \"tipo\" in df_todos.columns and \"variables\" in df_todos.columns:\n",
        "    mask_serie = df_todos[\"tipo\"].astype(str).str.strip().str.lower() == \"serie unica\"\n",
        "    if mask_serie.any():\n",
        "        vars_list = df_todos.loc[mask_serie, \"variables\"].apply(parse_variables_cell)\n",
        "        max_vars = int(vars_list.map(lambda lst: len(lst) if isinstance(lst, list) else 0).max())\n",
        "        new_cols = []\n",
        "        for i in range(1, max_vars + 1):\n",
        "            for base in (\"Variable_Valor\", \"Variable_Nombre\", \"Variable_Simbolo\"):\n",
        "                col = f\"{base}_{i}\"\n",
        "                new_cols.append(col)\n",
        "                if col not in df_todos.columns:\n",
        "                    df_todos[col] = pd.NA\n",
        "        def expand_row(lst, max_n):\n",
        "            data = {}\n",
        "            if not isinstance(lst, list):\n",
        "                return pd.Series({c: pd.NA for c in new_cols})\n",
        "            for i in range(min(max_n, len(lst))):\n",
        "                item = lst[i] if isinstance(lst[i], dict) else {}\n",
        "                data[f\"Variable_Valor_{i+1}\"]  = item.get(\"valor\", pd.NA)\n",
        "                data[f\"Variable_Nombre_{i+1}\"] = item.get(\"nombre\", pd.NA)\n",
        "                data[f\"Variable_Simbolo_{i+1}\"]= item.get(\"simbolo\", pd.NA)\n",
        "            for i in range(len(lst)+1, max_n+1):\n",
        "                data.setdefault(f\"Variable_Valor_{i}\", pd.NA)\n",
        "                data.setdefault(f\"Variable_Nombre_{i}\", pd.NA)\n",
        "                data.setdefault(f\"Variable_Simbolo_{i}\", pd.NA)\n",
        "            return pd.Series(data)\n",
        "        expanded = vars_list.apply(lambda lst: expand_row(lst, max_vars))\n",
        "        df_todos.loc[mask_serie, expanded.columns] = expanded.values\n",
        "\n",
        "# -------------------------\n",
        "# 6B) Multiserie (optimizado)\n",
        "# -------------------------\n",
        "if \"tipo\" in df_todos.columns and \"series\" in df_todos.columns:\n",
        "    mask_multi = df_todos[\"tipo\"].astype(str).str.strip().str.lower() == \"multiserie\"\n",
        "    if mask_multi.any():\n",
        "        series_list = df_todos.loc[mask_multi, \"series\"].apply(parse_multiserie_cell)\n",
        "\n",
        "        # Máximo de series y variables por serie\n",
        "        max_series = int(series_list.map(lambda lst: len(lst) if isinstance(lst, list) else 0).max())\n",
        "        max_vars_per_series = {i: 0 for i in range(1, max_series+1)}\n",
        "        for lst in series_list:\n",
        "            if isinstance(lst, list):\n",
        "                for idx, serie in enumerate(lst, start=1):\n",
        "                    vars_len = len(serie.get(\"variables\", [])) if isinstance(serie.get(\"variables\"), list) else 0\n",
        "                    if vars_len > max_vars_per_series[idx]:\n",
        "                        max_vars_per_series[idx] = vars_len\n",
        "\n",
        "        # Crear columnas necesarias\n",
        "        new_cols_multi = []\n",
        "        for s in range(1, max_series + 1):\n",
        "            new_cols_multi += [f\"Meta_{s}\", f\"Nombre_{s}\", f\"Resultado_{s}\"]\n",
        "            for v in range(1, max_vars_per_series[s] + 1):\n",
        "                new_cols_multi += [\n",
        "                    f\"Var_Valor_{s}_{v}\",\n",
        "                    f\"Var_Nombre_{s}_{v}\",\n",
        "                    f\"Var_Simbolo_{s}_{v}\"\n",
        "                ]\n",
        "        for col in new_cols_multi:\n",
        "            if col not in df_todos.columns:\n",
        "                df_todos[col] = pd.NA\n",
        "\n",
        "        # Expansión por fila\n",
        "        def expand_multiserie_row(lst):\n",
        "            data = {}\n",
        "            if not isinstance(lst, list):\n",
        "                return pd.Series({c: pd.NA for c in new_cols_multi})\n",
        "            for s_idx, serie in enumerate(lst, start=1):\n",
        "                data[f\"Meta_{s_idx}\"] = serie.get(\"meta\", pd.NA)\n",
        "                data[f\"Nombre_{s_idx}\"] = serie.get(\"nombre\", pd.NA)\n",
        "                data[f\"Resultado_{s_idx}\"] = serie.get(\"resultado\", pd.NA)\n",
        "                variables = serie.get(\"variables\", [])\n",
        "                if not isinstance(variables, list):\n",
        "                    variables = []\n",
        "                for v_idx, var in enumerate(variables, start=1):\n",
        "                    data[f\"Var_Valor_{s_idx}_{v_idx}\"] = var.get(\"valor\", pd.NA)\n",
        "                    data[f\"Var_Nombre_{s_idx}_{v_idx}\"] = var.get(\"nombre\", pd.NA)\n",
        "                    data[f\"Var_Simbolo_{s_idx}_{v_idx}\"] = var.get(\"simbolo\", pd.NA)\n",
        "            return pd.Series(data)\n",
        "\n",
        "        expanded_multi = series_list.apply(expand_multiserie_row)\n",
        "        df_todos.loc[mask_multi, expanded_multi.columns] = expanded_multi.values\n",
        "\n",
        "# 7) Guardar resultado\n",
        "df_todos.to_excel(\"Consolidado_Serie_Multi.xlsx\", index=False)\n",
        "print(\"✅ Consolidado.xlsx generado con columnas optimizadas para 'Serie Unica' y 'Multiserie'.\")"
      ],
      "metadata": {
        "id": "N8wYibrI82Il",
        "outputId": "31b3397f-cf9f-4ee6-dcfe-0e9dcb78bddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Consolidado.xlsx generado con columnas optimizadas para 'Serie Unica' y 'Multiserie'.\n"
          ]
        }
      ]
    }
  ]
}